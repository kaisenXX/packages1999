import os
import glob
import random
import numpy as np
from PIL import Image, ImageFilter
from tqdm import tqdm

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, random_split
import torchvision.transforms as T
import torchvision.models as models


# =====================
# Config (no CLI args)
# =====================
DATA_DIR = "./aircraft_crops"             # put your .jpeg crops here
PRETRAINED_VGG = "./pretrained/vgg19-dcbb9e9d.pth"
CKPT_DIR = "./checkpoints"
LOG_DIR = "./sr_logs"
RESULT_DIR = "./sr_results"
HR_SIZE = 100                              # your crop native size
SCALE = 4                                  # 4x super-resolution
BATCH_SIZE = 8
EPOCHS = 5
LR = 1e-4
USE_DEGRADE = True                         # use blur+noise degradation
VAL_SPLIT = 0.2
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"


# =====================
# Dataset (with degrade)
# =====================
class AircraftDataset(Dataset):
    def __init__(self, root, hr_size=HR_SIZE, scale=SCALE, use_degrade=USE_DEGRADE):
        self.files = sorted(
            glob.glob(os.path.join(root, "*.jpeg"))
            + glob.glob(os.path.join(root, "*.jpg"))
            + glob.glob(os.path.join(root, "*.png"))
        )
        self.hr_size = hr_size
        self.scale = scale
        self.use_degrade = use_degrade

        self.hr_transform = T.Compose([
            T.Resize((hr_size, hr_size), interpolation=Image.BICUBIC),
            T.ToTensor()
        ])
        # simple bicubic LR (baseline) — returns LR at hr_size//scale (NOT upsampled here)
        self.lr_bicubic = T.Compose([
            T.Resize((hr_size // scale, hr_size // scale), interpolation=Image.BICUBIC),
            T.ToTensor()
        ])

    def degrade(self, hr_tensor):
        """
        hr_tensor: torch.FloatTensor [3, H, W] in [0,1]
        Returns LR tensor of shape [3, H/scale, W/scale], with blur+noise.
        """
        # to PIL for filters
        pil_hr = T.ToPILImage()(hr_tensor)

        # Downsample to LR
        lr_w = self.hr_size // self.scale
        lr_h = self.hr_size // self.scale
        pil_lr = pil_hr.resize((lr_w, lr_h), resample=Image.BICUBIC)

        # Random blur
        if random.random() < 0.5:
            pil_lr = pil_lr.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.2, 1.2)))

        # Back to tensor
        lr_tensor = T.ToTensor()(pil_lr)

        # Add Gaussian noise
        if random.random() < 0.5:
            noise = torch.randn_like(lr_tensor) * 0.05
            lr_tensor = torch.clamp(lr_tensor + noise, 0.0, 1.0)

        return lr_tensor

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        # Force RGB (your originals are grayscale JPEGs; this just makes 3 channels)
        img = Image.open(self.files[idx]).convert("RGB")

        # HR is the "target" resolution (e.g., 100x100)
        hr = self.hr_transform(img)  # [3, HR, HR]

        # LR made from HR by downsampling (and optional degradation)
        if self.use_degrade:
            lr = self.degrade(hr)    # [3, HR/4, HR/4]
        else:
            lr = self.lr_bicubic(img)  # [3, HR/4, HR/4]

        return lr, hr


# =====================
# Model (residual SR, RGB in/out)
# =====================
class ResidualBlock(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)

    def forward(self, x):
        return x + self.conv2(self.relu(self.conv1(x)))


class SRNet(nn.Module):
    def __init__(self, num_residuals=8):
        super().__init__()
        self.entry = nn.Conv2d(3, 64, 3, padding=1)  # ✅ RGB input
        self.res_blocks = nn.Sequential(*[ResidualBlock(64) for _ in range(num_residuals)])

        # 2× upsampling
        self.upsample2 = nn.Sequential(
            nn.Conv2d(64, 256, 3, padding=1),
            nn.PixelShuffle(2),
            nn.ReLU(True)
        )
        self.exit2 = nn.Conv2d(64, 3, 3, padding=1)  # ✅ output 2× RGB

        # 4× upsampling
        self.upsample4 = nn.Sequential(
            nn.Conv2d(64, 256, 3, padding=1),
            nn.PixelShuffle(2),
            nn.ReLU(True)
        )
        self.exit4 = nn.Conv2d(64, 3, 3, padding=1)  # ✅ output 4× RGB

    def forward(self, x):
        # x: [B, 3, HR/4, HR/4]
        feat = self.entry(x)
        feat = self.res_blocks(feat)

        # 2× SR
        feat2 = self.upsample2(feat)   # [B, 64, HR/2, HR/2]
        out2 = self.exit2(feat2)       # [B, 3, HR/2, HR/2]

        # 4× SR
        feat4 = self.upsample4(feat2)  # [B, 64, HR, HR]
        out4 = self.exit4(feat4)       # [B, 3, HR, HR]

        return out2, out4

# =====================
# Perceptual loss (VGG19)
# =====================
class VGGPerceptualLoss(nn.Module):
    def __init__(self, vgg_path):
        super().__init__()
        vgg19 = models.vgg19()
        vgg19.load_state_dict(torch.load(r"C:\Users\sachi\Desktop\New folder (9)\pretrained\vgg19-dcbb9e9d.pth"))
        features = list(vgg19.features)[:36]  # up to relu4_4
        self.vgg = nn.Sequential(*features).eval()
        for p in self.vgg.parameters():
            p.requires_grad = False

    def forward(self, sr, hr):
    # expects RGB in [0,1], any size >= ~32
        f_sr = self.vgg(sr)
        f_hr = self.vgg(hr)

    # Debug: check feature shapes and values
        if not hasattr(self, "_printed"):
            print(f"[DEBUG] VGG Features SR: {f_sr.shape}, HR: {f_hr.shape}")
            print(f"[DEBUG] Example values SR mean={f_sr.mean().item():.4f}, HR mean={f_hr.mean().item():.4f}")
            self._printed = True  # only once

        return F.l1_loss(f_sr, f_hr)


# =====================
# Utils
# =====================
def save_image_grid(lr, sr2, sr4, hr, epoch, folder=LOG_DIR):
    os.makedirs(folder, exist_ok=True)
    # Upsample LR and SR2 to HR size so we can visualize side-by-side
    H = hr.shape[-2]
    W = hr.shape[-1]
    lr_up = F.interpolate(lr, size=(H, W), mode="bicubic", align_corners=False)
    sr2_up = F.interpolate(sr2, size=(H, W), mode="bicubic", align_corners=False)

    # Take first item of batch and stack horizontally [LR | SR2 | SR4 | HR]
    grid = torch.cat([lr_up[0], sr2_up[0], sr4[0], hr[0]], dim=-1)  # along width
    grid = (grid.clamp(0, 1).permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)
    Image.fromarray(grid).save(os.path.join(folder, f"epoch_{epoch:04d}.jpeg"))
    
    
    
def sobel_edges(x):
    """Apply Sobel filter to detect edges (horizontal + vertical)."""
    x_gray = torch.mean(x, dim=1, keepdim=True)  # grayscale
    sobel_x = torch.tensor([[-1,0,1],[-2,0,2],[-1,0,1]],
                           dtype=torch.float32, device=x.device).unsqueeze(0).unsqueeze(0)
    sobel_y = torch.tensor([[-1,-2,-1],[0,0,0],[1,2,1]],
                           dtype=torch.float32, device=x.device).unsqueeze(0).unsqueeze(0)
    grad_x = F.conv2d(x_gray, sobel_x, padding=1)
    grad_y = F.conv2d(x_gray, sobel_y, padding=1)
    return torch.sqrt(grad_x ** 2 + grad_y ** 2 + 1e-6)

def laplacian_edges(x):
    """Apply Laplacian filter to detect edges in all directions."""
    x_gray = torch.mean(x, dim=1, keepdim=True)  # grayscale
    laplacian_kernel = torch.tensor([[0,1,0],
                                     [1,-4,1],
                                     [0,1,0]],
                                    dtype=torch.float32, device=x.device).unsqueeze(0).unsqueeze(0)
    return F.conv2d(x_gray, laplacian_kernel, padding=1)


# =====================
# Train
# =====================
def train():
    device = DEVICE
    dataset = AircraftDataset(DATA_DIR, hr_size=HR_SIZE, scale=SCALE, use_degrade=USE_DEGRADE)
    n_total = len(dataset)
    n_val = max(1, int(n_total * VAL_SPLIT))
    n_train = n_total - n_val
    train_ds, val_ds = random_split(dataset, [n_train, n_val])

    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)

    model = SRNet().to(device)
    optimizer = optim.Adam(model.parameters(), lr=LR)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)
    vgg_loss_fn = VGGPerceptualLoss(PRETRAINED_VGG).to(device)
    l1 = nn.L1Loss()

    # --- Auto resume ---
    os.makedirs(CKPT_DIR, exist_ok=True)
    start_epoch = 0
    ckpts = sorted(glob.glob(os.path.join(CKPT_DIR, "*.pth")))
    if ckpts:
        latest = ckpts[-1]
        ckpt = torch.load(latest, map_location=device)
        model.load_state_dict(ckpt["model_state"])
        optimizer.load_state_dict(ckpt["optim_state"])
        scheduler.load_state_dict(ckpt["sched_state"])
        start_epoch = ckpt["epoch"] + 1
        print(f"[INFO] Resuming from {latest}, epoch {ckpt['epoch']}")
    else:
        print("[INFO] No checkpoint found, starting fresh training.")

    for epoch in range(start_epoch, EPOCHS):
        model.train()
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS}")

        for lr_img, hr_img in pbar:
            lr_img, hr_img = lr_img.to(device), hr_img.to(device)

        # Forward
            sr2, sr4 = model(lr_img)   # both are RGB now

        # Downsample HR for supervising 2× SR
            hr_half = F.interpolate(hr_img, scale_factor=0.5, mode="bicubic", align_corners=False)

        # Losses
            loss_l1 = l1(sr2, hr_half) + l1(sr4, hr_img)
            loss_vgg = vgg_loss_fn(sr4, hr_img)   # perceptual on final 4×

        # Edge-based losses
            edges_sr_sobel = sobel_edges(sr4)
            edges_hr_sobel = sobel_edges(hr_img)
            edges_sr_lap = laplacian_edges(sr4)
            edges_hr_lap = laplacian_edges(hr_img)

            loss_edge = F.l1_loss(edges_sr_sobel, edges_hr_sobel) + \
                        F.l1_loss(edges_sr_lap, edges_hr_lap)

        # Final combined loss (tweak weights here)
            loss = 0.5 * loss_l1 + 1 * loss_vgg + 0.7 * loss_edge

        # Backprop
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            pbar.set_postfix({"loss": f"{loss.item():.4f}"})

            
   

    

        # Validation preview & log image
        model.eval()
        with torch.no_grad():
            try:
                lr_v, hr_v = next(iter(val_loader))
                lr_v, hr_v = lr_v.to(device), hr_v.to(device)
                sr2_v, sr4_v = model(lr_v)
                save_image_grid(lr_v, sr2_v, sr4_v, hr_v, epoch+1)
            except StopIteration:
                pass

        # Save checkpoint every epoch
        ckpt_path = os.path.join(CKPT_DIR, f"ckpt_epoch{epoch+1:04d}.pth")
        torch.save({
            "epoch": epoch,
            "model_state": model.state_dict(),
            "optim_state": optimizer.state_dict(),
            "sched_state": scheduler.state_dict()
        }, ckpt_path)
        print(f"[INFO] Saved checkpoint: {ckpt_path}")


# =====================
# Inference
#  - Feed your original crop as LR; the network outputs 2× and 4× versions.
# =====================
def infer(img_path, ckpt_path, which="4x"):
    device = DEVICE
    model = SRNet().to(device)
    ckpt = torch.load(ckpt_path, map_location=device)
    model.load_state_dict(ckpt["model_state"])
    model.eval()

    img = Image.open(img_path).convert("RGB")
    lr = T.ToTensor()(img).unsqueeze(0).to(device)  # treat input as LR
    with torch.no_grad():
        sr2, sr4 = model(lr)
        sr = sr2 if which == "2x" else sr4

    sr = sr.clamp(0, 1)[0].permute(1, 2, 0).cpu().numpy()
    sr = (sr * 255).astype(np.uint8)
    os.makedirs(RESULT_DIR, exist_ok=True)
    out_path = os.path.join(RESULT_DIR, f"sr_{which}.jpg")
    Image.fromarray(sr).save(out_path)
    print(f"[INFO] Saved: {out_path}")


if __name__ == "__main__":
    # Train:
    train()

    # Inference example (uncomment to use after training):
    #infer("./aircraft_crops/some_crop.jpeg", "./checkpoints/ckpt_epoch0100.pth", which="4x")
